{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text 1115393\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read tiny shakespeare\n",
    "\n",
    "with open('tinyshakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'length of text {len(text)}')\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Here are all the unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "print(''.join(chars))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 6, 1, 40, 47, 58, 41, 46]\n",
      "hello, bitch\n"
     ]
    }
   ],
   "source": [
    "# create mapping from characters to integers\n",
    "string_to_index = {ch:i for i, ch in enumerate(chars)}\n",
    "index_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# encoder: convert string to list of int\n",
    "encode = lambda s : [string_to_index[c] for c in s] \n",
    "\n",
    "# decoder: \n",
    "decode = lambda l : ''.join([index_to_string[i] for i in l])\n",
    "\n",
    "print(encode('hello, bitch'))\n",
    "print(decode(encode('hello, bitch')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EddyM\\miniconda3\\envs\\ai-env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train - validation split\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[ : block_size]\n",
    "y = train_data[1 : block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[: t + 1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context} the target is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
      "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
      "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
      "        [39,  1, 46, 53, 59, 57, 43,  0]])\n",
      "-----\n",
      "When input is [53] the target: 59\n",
      "When input is [53, 59] the target: 6\n",
      "When input is [53, 59, 6] the target: 1\n",
      "When input is [53, 59, 6, 1] the target: 58\n",
      "When input is [53, 59, 6, 1, 58] the target: 56\n",
      "When input is [53, 59, 6, 1, 58, 56] the target: 47\n",
      "When input is [53, 59, 6, 1, 58, 56, 47] the target: 40\n",
      "When input is [53, 59, 6, 1, 58, 56, 47, 40] the target: 59\n",
      "When input is [49] the target: 43\n",
      "When input is [49, 43] the target: 43\n",
      "When input is [49, 43, 43] the target: 54\n",
      "When input is [49, 43, 43, 54] the target: 1\n",
      "When input is [49, 43, 43, 54, 1] the target: 47\n",
      "When input is [49, 43, 43, 54, 1, 47] the target: 58\n",
      "When input is [49, 43, 43, 54, 1, 47, 58] the target: 1\n",
      "When input is [49, 43, 43, 54, 1, 47, 58, 1] the target: 58\n",
      "When input is [13] the target: 52\n",
      "When input is [13, 52] the target: 45\n",
      "When input is [13, 52, 45] the target: 43\n",
      "When input is [13, 52, 45, 43] the target: 50\n",
      "When input is [13, 52, 45, 43, 50] the target: 53\n",
      "When input is [13, 52, 45, 43, 50, 53] the target: 8\n",
      "When input is [13, 52, 45, 43, 50, 53, 8] the target: 0\n",
      "When input is [13, 52, 45, 43, 50, 53, 8, 0] the target: 26\n",
      "When input is [1] the target: 39\n",
      "When input is [1, 39] the target: 1\n",
      "When input is [1, 39, 1] the target: 46\n",
      "When input is [1, 39, 1, 46] the target: 53\n",
      "When input is [1, 39, 1, 46, 53] the target: 59\n",
      "When input is [1, 39, 1, 46, 53, 59] the target: 57\n",
      "When input is [1, 39, 1, 46, 53, 59, 57] the target: 43\n",
      "When input is [1, 39, 1, 46, 53, 59, 57, 43] the target: 0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4  # how many independant sequences will we process in parallel\n",
    "block_size = 8  # what is the maximum context length for prefiction\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x & targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('-----')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f'When input is {context.tolist()} the target: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8948, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "\n",
    "        # index & targets are both (B, T) tensor of integers\n",
    "        # in our case \n",
    "        #   B = BatchSize = 4\n",
    "        #   T = Time (BlockSize) = 8\n",
    "        #   C = Channel (TokenSize) = 65\n",
    "        logits = self.token_embedding_table(index) # (B, T, C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(index)\n",
    "            # focus on only the last time step\n",
    "            logits = logits[:, -1, :] # Becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "index = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(index, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6]],\n",
      "\n",
      "        [[1, 1, 1],\n",
      "         [2, 2, 2]]])\n",
      "\n",
      "tensor([[4, 5, 6],\n",
      "        [2, 2, 2]])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([[[1,2,3], [4,5,6]], [[1,1,1], [2,2,2]]])\n",
    "print(logits)\n",
    "print()\n",
    "print(logits[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6662204265594482\n"
     ]
    }
   ],
   "source": [
    "# <-- keep hitting the play button here to lower loss\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for steps in range(1000):\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wh;;Sq.f ustNzknc\n",
      "kwgOj$dhPWr,SV?hsusiKpgXXUh;Apmem d?hESXI.i;TrJgkiF-oKbXCAA -botrngFCHAUQkn$\n",
      "\n",
      "pn$w-gHoi?wtd!\n",
      "LLULIfSK'bAw :M.ZtOptXEQcL?hfaofqbPd?OnonQQJMap$aypupIBYGUsZaI'ottllo..k$W$Akp?yl?ajKlzY!lx&QQLW? t,bXFkyhl-dmVsHUEcORl,jSClgjuk:3Iv\n",
      "?OqlrV;!Plxfzgy;;\n",
      "'mRWvkQ&xk!$\n",
      "h\n",
      "SiruDJgKuDny,S$ERf.?GSV\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(index = torch.zeros((1,1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 300\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "    out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The Mathematical Trick in self-attention</h3>\n",
    "Let's say, in one sample, for every t'th token, calculate avg. of all t tokens up until that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# A[b, t] = mean_{x <= t} x [b, i]\n",
    "A = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t + 1] # (t, C)\n",
    "        A[b, t] = torch.mean(xprev, 0)\n",
    "\n",
    "print(x[0])\n",
    "print()\n",
    "print(A[0])\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve this w/o using for-loops by using matrix multiplication. As a reminder:\n",
    "<br><br>![image.png](attachment:image.png)<br>\n",
    "<!-- $$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "7 & 8 \\\\\n",
    "9 & 10 \\\\\n",
    "11 & 12\n",
    "\\end{bmatrix}\n",
    "=\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IDENTITY\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = torch.matmul(a, b)\n",
    "print(a)\n",
    "print()\n",
    "print(b)\n",
    "print()\n",
    "print(c)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SUM\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = torch.matmul(a, b)\n",
    "print(a)\n",
    "print()\n",
    "print(b)\n",
    "print()\n",
    "print(c)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AVERAGE\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = torch.matmul(a, b)\n",
    "print(a)\n",
    "print()\n",
    "print(b)\n",
    "print()\n",
    "print(c)\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using weight to take average of all tokens up until any particular token. In the form of matrix, the token weights are uniform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "# # xbow = wei @ A    # (B, T, T) @ (B, T, C) ---> (B, T, C)\n",
    "print(wei)\n",
    "\n",
    "print()\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "print(tril)\n",
    "print() \n",
    "\n",
    "# Could also achieve with softmax\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T, C)\n",
    "        x = tok_emb + pos_emb\n",
    "        logits = self.lm_head(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Self-Attention</h3>\n",
    "Recall that we used uniform weights to find average of previous tokens. The average is an over-simplified way of recalling from the past. We would like to use different weights on different types of tokens' queries and keys. (eg. verb would look for subject). So we use linear layers to convert, and use multiplication to have keys and queries interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n",
      "\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)      # (B, T, 16)\n",
    "q = query(x)    # (B, T, 16)\n",
    "\n",
    "\n",
    "wei = q @ k.transpose(1, 2)     # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "'''\n",
    "and to prevent keys and queries from looking into the future,\n",
    "we use upper triangular masking\n",
    "'''\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "\n",
    "'''\n",
    "finally, we apply attention, this is telling us the data dependant \n",
    "manner, how much we'd like to aggregate from the past.\n",
    "\n",
    "But instead of raw x, we use a value, basically telling how much \n",
    "'''\n",
    "# out = wei @ x\n",
    "v = value(x)    # (B, T, 16)\n",
    "out = wei @ v\n",
    "\n",
    "print(out.shape)\n",
    "print()\n",
    "print(wei[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 121\u001b[0m\n\u001b[0;32m    118\u001b[0m model \u001b[39m=\u001b[39m GPT0(vocab_size)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    120\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m5\u001b[39m,\u001b[39m6\u001b[39m,\u001b[39m7\u001b[39m], [\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m]])\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m y \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m    122\u001b[0m \u001b[39mprint\u001b[39m(y)\n",
      "File \u001b[1;32mc:\\Users\\EddyM\\miniconda3\\envs\\ai-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[26], line 85\u001b[0m, in \u001b[0;36mGPT0.forward\u001b[1;34m(self, index, target)\u001b[0m\n\u001b[0;32m     82\u001b[0m B, T \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39mshape\n\u001b[0;32m     84\u001b[0m token_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_embedding_table(index) \u001b[39m# (B, T, C)\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m posit_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mposition_embedding_table(torch\u001b[39m.\u001b[39;49marange(T)) \u001b[39m# (T, C)\u001b[39;00m\n\u001b[0;32m     87\u001b[0m x \u001b[39m=\u001b[39m token_embed \u001b[39m+\u001b[39m posit_embed   \u001b[39m# (B, T, C)\u001b[39;00m\n\u001b[0;32m     88\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks(x)\n",
      "File \u001b[1;32mc:\\Users\\EddyM\\miniconda3\\envs\\ai-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\EddyM\\miniconda3\\envs\\ai-env\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\EddyM\\miniconda3\\envs\\ai-env\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)     # (B, T, C)\n",
    "        q = self.query(x)   # (B, T, C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5   # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)    # (B, T, T)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)   # (B, T, C)\n",
    "        out = wei @ v   # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embed) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, drop_out):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embed, drop_out)\n",
    "        self.ffwd = FeedFoward(n_embed, drop_out)\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embed)\n",
    "        self.layer_norm2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.layer_norm1(x))\n",
    "        x = x + self.ffwd(self.layer_norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT0(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed=384, n_head=6, n_layer=6, drop_out=0.2):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head, drop_out) for _ in range(n_layer)])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, index, target=None):\n",
    "        B, T = index.shape\n",
    "\n",
    "        token_embed = self.token_embedding_table(index) # (B, T, C)\n",
    "        posit_embed = self.position_embedding_table(torch.arange(T)) # (T, C)\n",
    "\n",
    "        x = token_embed + posit_embed   # (B, T, C)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            target = target.view(B * T)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop index to the last block_size tokens\n",
    "            index_cond = index[:, -block_size:]\n",
    "            # get predictions\n",
    "            logits, loss = self(index_cond)\n",
    "            # focus on only the last time step\n",
    "            logits = logits[:, -1, :] # Becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = GPT0(vocab_size).to('cuda')\n",
    "\n",
    "# x = torch.tensor([[1,2,3,4,5,6,7], [1,1,1,1,1,1,1]]).to('cuda')\n",
    "# y = model(x)\n",
    "# print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iters):\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m     \u001b[39m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39m%\u001b[39m eval_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39m==\u001b[39m max_iters \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m----> 8\u001b[0m         losses \u001b[39m=\u001b[39m estimate_loss()\n\u001b[0;32m      9\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstep \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39miter\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: train loss \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, val loss \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[39m# sample a batch of data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\EddyM\\miniconda3\\envs\\ai-env\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[23], line 51\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(eval_iters):\n\u001b[0;32m     50\u001b[0m     X, Y \u001b[39m=\u001b[39m get_batch(split)\n\u001b[1;32m---> 51\u001b[0m     logits, loss \u001b[39m=\u001b[39m model(X, Y)\n\u001b[0;32m     52\u001b[0m     losses[k] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     53\u001b[0m out[split] \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\EddyM\\miniconda3\\envs\\ai-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[22], line 85\u001b[0m, in \u001b[0;36mGPT0.forward\u001b[1;34m(self, index, target)\u001b[0m\n\u001b[0;32m     82\u001b[0m B, T \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39mshape\n\u001b[0;32m     84\u001b[0m token_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_embedding_table(index) \u001b[39m# (B, T, C)\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m posit_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mposition_embedding_table(torch\u001b[39m.\u001b[39;49marange(T)) \u001b[39m# (T, C)\u001b[39;00m\n\u001b[0;32m     87\u001b[0m x \u001b[39m=\u001b[39m token_embed \u001b[39m+\u001b[39m posit_embed   \u001b[39m# (B, T, C)\u001b[39;00m\n\u001b[0;32m     88\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks(x)\n",
      "File \u001b[1;32mc:\\Users\\EddyM\\miniconda3\\envs\\ai-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\EddyM\\miniconda3\\envs\\ai-env\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\EddyM\\miniconda3\\envs\\ai-env\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f331462f1a3623d9ad8b70946973c1dbbaf4cf768e0f82a62f2d70059232a38a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
